import json
import os
import sqlite3

from dotenv import load_dotenv
from jinja2 import Environment, FileSystemLoader, Template
from openai import OpenAI

load_dotenv(".env")

llm_client = OpenAI(
    base_url="https://api.studio.nebius.ai/v1/",
    api_key=os.environ.get("NEBIUS_API_KEY"),
)

file_loader = FileSystemLoader("/Users/stas_chi/Documents/Projects/Izba AI/izba-ai/backend/prompts")  # Specify the directory containing your template file
env = Environment(loader=file_loader)

def historical_data_tool_func(user_query: str) -> dict[str,str]:
    """Tool that has access to all historical transactions of UK properties since 1995.

    Takes in a user query as an input and returns the output in the following format:
    {
        "debug_info": any debugging info available,
        "llm_response": final response generated by the LLM
    }
    """
    template = env.get_template('historical_data_to_sql.jinja2')
    # Render the template
    prompt = template.render(
        user_query = user_query
    )

    completion = llm_client.chat.completions.create(
        model="meta-llama/Meta-Llama-3.1-405B-Instruct",
        messages=[
        {
            "role": "system",
            "content": prompt
        }
        ],
        temperature=0,
        max_tokens=1000,
        top_p=0.9
    )

    try:
        SQL_QUERY = json.loads(completion.choices[0].message.content)["SQL_QUERY"]
    except:
        print("Failed to load JSON from LLM output")

    # Step 1: Connect to the SQLite database (or create it if it doesn't exist)
    conn = sqlite3.connect('/Users/stas_chi/Documents/Projects/Izba AI/izba-ai/backend/databases/price_paid_data.db')  # Replace with your database file

    # Step 2: Create a cursor object
    cursor = conn.cursor()

    # Step 3: Run a SELECT query to fetch data
    cursor.execute(SQL_QUERY)

    # Step 4: Process and print the results
    rows = cursor.fetchall()

    # Print the results
    db_response = str(rows[0][0])

    # Step 5: Close the connection
    conn.close()

    # Render the template
    template_final_response = env.get_template('historical_data_final_response.jinja2')
    prompt = template_final_response.render(
        user_query = user_query,
        sql_query = SQL_QUERY,
        db_response = db_response
    )

    completion = llm_client.chat.completions.create(
        model="meta-llama/Meta-Llama-3.1-405B-Instruct",
        messages=[
        {
            "role": "system",
            "content": prompt
        }
        ],
        temperature=0,
        max_tokens=1000,
        top_p=0.9
    )

    return {
        "debug_info": SQL_QUERY,
        "llm_response": completion.choices[0].message.content
    }